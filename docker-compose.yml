services:
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - "3001:8080"
    environment:
      OLLAMA_BASE_URL: "http://host.docker.internal:11434"  # Communique avec le service Ollama
      OPENAI_API_URL: "http://pipelines:9099"  # Communique avec le service Pipelines
    volumes:
      - openwebui-data:/app/backend/data
      - ./open-webui:/app/backend/data  # Persistance des données localement
    depends_on:
      - pipelines
    networks:
      - webui-net

  pipelines:
    image: ghcr.io/open-webui/pipelines:main
    ports:
      - "9099:9099"
    volumes:
      - ./pipelines:/app/pipelines
      - ./data:/app/data
      - ./db:/app/db
      - ./src:/app/src
      - shared_data:/srv
      - ./data:/srv/data
      - ./Classifieur_Model/classifieur.pt:/app/share/classifieur.pt
      - ./rag:/app/rag
      - openwebui-data:/app/backend/data
      - ./open-webui:/app/backend/data  # Persistance des données localement
    environment:
      - PIPELINES_DIR=/app/pipelines
      - PIPELINES_REQUIREMENTS_PATH=/app/pipelines/requirements.txt
      - PYTHONPATH=/app/src
    networks:
      - webui-net

  filebrowser:
    image: filebrowser/filebrowser
    container_name: filebrowser
    volumes:
      - shared_data:/srv
      - ./data/:/srv/data
    ports:
      - "8080:80"
    environment:
      - FB_NOAUTH=true

networks:
  webui-net:

volumes:
  ollama:
    driver: local
  shared_data:
    driver: local
  openwebui-data:
    driver: local
